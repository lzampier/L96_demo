{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a013986b",
   "metadata": {},
   "source": [
    "# Data Assimilation demo in the Lorenz 96 (L96) two time-scale model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c383a2a",
   "metadata": {},
   "source": [
    "# Recap of L96 and notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a10bb",
   "metadata": {},
   "source": [
    "{cite:t}`Lorenz1995` describes a \"two time-scale\" model in two equations (2 and 3) which are:\n",
    "\\begin{align}\n",
    "\\frac{d}{dt} X_k\n",
    "&= - X_{k-1} \\left( X_{k-2} - X_{k+1} \\right) - X_k + F - \\left( \\frac{hc}{b} \\right) \\sum_{j=0}^{J-1} Y_{j,k}\n",
    "\\\\\n",
    "\\frac{d}{dt} Y_{j,k}\n",
    "&= - cbY_{j+1,k} \\left( Y_{j+2,k} - X_{j-1,k} \\right) - c Y_{j,k} + \\frac{hc}{b} X_k\n",
    "\\end{align}\n",
    "Note that the $F$ term in the top equation is missing in L96 but he provides the parameter and all other authors have it, e.g. {cite:t}`Wilks2005`. $X_k$ are the \"large scale\" with $K$ degrees of freedom ($k=0,1,2,\\ldots,K-1$, using python indexing starting at $k=0$). The $k$ index is periodic so that, for example, $k=K$ is referring to $k=0$ and $k=-1$ is referring to $k=K-1$. The $j$ indices represent a sub-division of each $k$-element, so that $J$ Y-values are coupled to a single $X$ value. When $j+1,k$ refers to a values beyond $J$, it is cycled to refer to the first value of with $1,k+1$. The overall structure is illustrated in Fig. 1.\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/319201436/figure/fig1/AS:869115023589376@1584224577926/Visualisation-of-a-two-scale-Lorenz-96-system-with-J-8-and-K-6-Global-scale-values.png\" width=400> *Fig. 1: Visualisation of a two-scale Lorenz '96 system with J = 8 and K = 6. Global-scale values (X k ) are updated based on neighbouring values and a reduction applied to the local-scale values (Y j,k ) associated with that value. Local-scale values are updated based on neighbouring values and the associated global-scale value. The neighbourhood topology of both the local and global-scale values is circular. Image from {cite:t}`Russell2017`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66584b6f",
   "metadata": {},
   "source": [
    "# 1. Define variables and functions to use throughout notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from L96_model import L96, RK4, L96_2t_xdot_ydot, L96_eq1_xdot, L96s\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "config = dict(\n",
    "    K=40,  # Dimension of L96 \"X\" variables\n",
    "    J=10,  # Dimension of L96 \"Y\" variables\n",
    "    obs_freq=10,  # observation frequency (number of sampling intervals (si) per observation)\n",
    "    F_truth=10,  # F for truth signal\n",
    "    F_fcst=10,  # F for forecast (DA) model\n",
    "    GCM_param=np.array(\n",
    "        [0, 0, 0, 0]\n",
    "    ),  # polynomial coefficicents for GCM parameterization\n",
    "    ns_da=2000,  # number of time samples for DA\n",
    "    ns=2000,  # number of time samples for truth signal\n",
    "    ns_spinup=200,  # number of time samples for spin up\n",
    "    dt=0.005,  # model timestep\n",
    "    si=0.005,  # truth sampling interval\n",
    "    seasonal=False,  # option for adding a seasonal cycle to the forcing in the L96 truth model\n",
    "    B_loc=5,  # spatial localization radius for DA\n",
    "    DA=\"EnKF\",  # DA method\n",
    "    nens=100,  # number of ensemble members for DA\n",
    "    inflate_opt=\"relaxation\",  # method for DA model covariance inflation\n",
    "    inflate_factor=0.2,  # inflation factor\n",
    "    hybrid_factor=0.1,  # inflation factor for hybrid EnKF method\n",
    "    param_DA=False,  # switch to to parameter estimation with DA\n",
    "    param_sd=[0.01, 0.02, 0.1, 0.5],  # polynomal parameter standard deviation\n",
    "    param_inflate=\"multiplicative\",  # method for parameter variance inflation\n",
    "    param_inf_factor=0.02,  # parameter inflation factor\n",
    "    obs_density=0.2,  # fraction of spatial gridpoints where observations are collected\n",
    "    DA_freq=10,  # assimilation frequency (number of sampling intervals (si) per assimilation step)\n",
    "    obs_sigma=0.5,  # observational error standard deviation\n",
    "    save_fig=False,  # switch to save figure file\n",
    "    save_data=False,  #  switch to save\n",
    ")\n",
    "\n",
    "\n",
    "def GCM(X0, F, dt, nt, param=[0]):\n",
    "    \"\"\"\n",
    "    2 time-scale model with a parameterised coupling term to represent the interaction\n",
    "    between the observed coarse scale processes (X) and unobserved fine scale processes (Y).\n",
    "    Inputs:\n",
    "        X0: Initial conditions of X (vector of length config['K'])\n",
    "        F: Forcing (scalar)\n",
    "        dt: Sampling frequency (timestep) of the model (scalar)\n",
    "        nt: Number of timesteps for which to run the model (scalar)\n",
    "        param: Weights to give to the coupling term\n",
    "    Returns:\n",
    "        hist: Model output at each timestep (nt + 1 x config['K'])\n",
    "        time: Corresponding time units for model outputs (nt + 1)\n",
    "    \"\"\"\n",
    "    time, hist, X = (\n",
    "        dt * np.arange(nt + 1),\n",
    "        np.zeros((nt + 1, len(X0))) * np.nan,\n",
    "        X0.copy(),\n",
    "    )\n",
    "    hist[0] = X\n",
    "\n",
    "    for n in range(nt):\n",
    "        X = X + dt * (L96_eq1_xdot(X, F) - np.polyval(param, X))\n",
    "\n",
    "        hist[n + 1], time[n + 1] = X, dt * (n + 1)\n",
    "    return hist, time\n",
    "\n",
    "\n",
    "def ObsOp(K, l_obs, t_obs, i_t):\n",
    "    \"\"\"\n",
    "    Observation operator to map between model and observation space,\n",
    "    assuming linearity and model space observations.\n",
    "    Inputs:\n",
    "        K: spatial dimension of the model (scalar)\n",
    "        l_obs: spatial positions of observations on model grid (config['ns_DA']/config['obs_freq'] x config['K']*config['obs_density'])\n",
    "        t_obs: time positions of observations (config['ns_DA']/config['obs_freq'] x config['K']*config['obs_density'])\n",
    "        i_t: the timestep of the current DA cycle (scalar)\n",
    "    Returns:\n",
    "        H: Operator matrix (config['K']*config['obs_density'] x config['K'])\n",
    "    \"\"\"\n",
    "    n = l_obs.shape[-1]\n",
    "    H = np.zeros((n, K))\n",
    "    H[range(n), l_obs[t_obs == i_t]] = 1\n",
    "    return H\n",
    "\n",
    "\n",
    "def cov_loc(B, loc=0):\n",
    "    \"\"\"\n",
    "    Localize the model climatology covariance matrix, based on\n",
    "    the Gaspari-Cohn function\n",
    "    Inputs:\n",
    "        B: Covariance matrix over a long model run 'M_truth' (config['K'] x config['K'])\n",
    "        loc: spatial localization radius for DA (scalar)\n",
    "    Returns:\n",
    "        B*W: Covariance matrix scaled to zero outside distance 'loc' from diagonal (config['K'] x config['K'])\n",
    "        W: Matrix of weights which are used to scale covariance matrix (config['K'] x config['K'])\n",
    "    \"\"\"\n",
    "    M, N = B.shape\n",
    "    X, Y = np.ix_(np.arange(M), np.arange(N))\n",
    "    dist = np.vectorize(get_dist)(X, Y, M)\n",
    "    W = np.vectorize(gaspari_cohn)(dist, loc)\n",
    "    return B * W, W\n",
    "\n",
    "\n",
    "def get_dist(i, j, K):\n",
    "    \"\"\"\n",
    "    Compute the absolute distance between two element indices\n",
    "    within a square matrix of size (K x K)\n",
    "    Inputs:\n",
    "        i: the ith row index (scalar)\n",
    "        j: the jth column index (scalar)\n",
    "        K: shape of square array (scalar)\n",
    "    Returns:\n",
    "        Distance (scalar)\n",
    "    \"\"\"\n",
    "    return abs(i - j) if abs(i - j) <= K / 2 else K - abs(i - j)\n",
    "\n",
    "\n",
    "def gaspari_cohn(distance, radius):\n",
    "    \"\"\"\n",
    "    Compute the appropriate distance dependent weighting of a\n",
    "    covariance matrix, after Gaspari & Cohn, 1999 (https://doi.org/10.1002/qj.49712555417)\n",
    "    Inputs:\n",
    "        distance: from get_dist(), the distance between array elements (scalar)\n",
    "        radius: localization radius for DA (scalar)\n",
    "    Returns:\n",
    "        weight: distance dependent weight of the ith,jth index of a covariance matrix (scalar)\n",
    "    \"\"\"\n",
    "    if distance == 0:\n",
    "        weight = 1.0\n",
    "    else:\n",
    "        if radius == 0:\n",
    "            weight = 0.0\n",
    "        else:\n",
    "            ratio = distance / radius\n",
    "            weight = 0.0\n",
    "            if ratio <= 1:\n",
    "                weight = (\n",
    "                    -(ratio**5) / 4\n",
    "                    + ratio**4 / 2\n",
    "                    + 5 * ratio**3 / 8\n",
    "                    - 5 * ratio**2 / 3\n",
    "                    + 1\n",
    "                )\n",
    "            elif ratio <= 2:\n",
    "                weight = (\n",
    "                    ratio**5 / 12\n",
    "                    - ratio**4 / 2\n",
    "                    + 5 * ratio**3 / 8\n",
    "                    + 5 * ratio**2 / 3\n",
    "                    - 5 * ratio\n",
    "                    + 4\n",
    "                    - 2 / 3 / ratio\n",
    "                )\n",
    "    return weight\n",
    "\n",
    "\n",
    "def find_obs(loc, obs, t_obs, l_obs, period):\n",
    "    \"\"\"\n",
    "    This function is for plotting purposes only.\n",
    "    \"\"\"\n",
    "    t_period = np.where((t_obs[:, 0] >= period[0]) & (t_obs[:, 0] < period[1]))\n",
    "    obs_period = np.zeros(t_period[0].shape)\n",
    "    obs_period[:] = np.nan\n",
    "    for i in np.arange(len(obs_period)):\n",
    "        if np.any(l_obs[t_period[0][i]] == loc):\n",
    "            obs_period[i] = obs[t_period[0][i]][l_obs[t_period[0][i]] == loc]\n",
    "    return obs_period\n",
    "\n",
    "\n",
    "def running_ave(X, N):\n",
    "    \"\"\"\n",
    "    Compute running mean over a user-specified window\n",
    "    Input:\n",
    "        X: Input vector of arbitrary length 'n'\n",
    "        N: Size of window over which to compute mean (integer scalar)\n",
    "    Returns:\n",
    "        X_sum/N: X averaged over window N\n",
    "    \"\"\"\n",
    "    if N % 2 == 0:\n",
    "        N1, N2 = -N / 2, N / 2\n",
    "    else:\n",
    "        N1, N2 = -(N - 1) / 2, (N + 1) / 2\n",
    "    X_sum = np.zeros(X.shape)\n",
    "    for i in np.arange(N1, N2):\n",
    "        X_sum = X_sum + np.roll(X, int(i), axis=0)\n",
    "    return X_sum / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3052af",
   "metadata": {},
   "source": [
    "# 2. Generate 'truth' run from two time-scale L96 model\n",
    "\n",
    "Here we initialise the L96 two time-scale model using a set of random normally distributed values for X, and zeros for Y, and run it forward for a period 'ns_spinup' to allow the model to spinup. Following this, the spunup X and Y components are used as initial conditions for another forward run for a time period 'ns' to represent the unobserved 'truth' field from which our observations will be derived.\n",
    "\n",
    "Note that in reality we do not observe the fine scale components (the Ys) and so this is what we aim to represent in the parameterisation in the GCM() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b5cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the \"truth\" 2-scale L96 model and generate initial conditions from a short spinup\n",
    "M_truth = L96(\n",
    "    config[\"K\"], config[\"J\"], F=config[\"F_truth\"], dt=config[\"dt\"]\n",
    ")  # initialise L96 class\n",
    "M_truth.set_state(\n",
    "    rng.standard_normal((config[\"K\"])), 0 * M_truth.j\n",
    ")  # set initial conditions of X and Y\n",
    "X_spinup, Y_spinup, t_spinup = M_truth.run(\n",
    "    config[\"si\"], config[\"si\"] * config[\"ns_spinup\"]\n",
    ")  # let the model run for 'ns_spinup' number of time steps to spin-up\n",
    "X_init = X_spinup[\n",
    "    -1, :\n",
    "]  # the initial conditions of X for the first forecast (prior to DA) are the last time sample after spinup\n",
    "Y_init = Y_spinup[-1, :]  # similar for Y\n",
    "\n",
    "# Run L96 to generate the \"truth\"\n",
    "M_truth.set_state(X_init, Y_init)\n",
    "\n",
    "# # Give F a \"seasonal cycle\" in the truth model\n",
    "if config[\"seasonal\"]:\n",
    "    ann_period = 2000\n",
    "    mon_period = 100\n",
    "    mon_per_ann = ann_period / mon_period\n",
    "    X_truth, Y_truth, t_truth = M_truth.run(config[\"si\"], config[\"si\"] * mon_period)\n",
    "    for i in range(1, int(config[\"ns\"] / mon_period)):\n",
    "        M_truth.set_state(X_truth[-1, ...], Y_truth[-1, ...])\n",
    "        M_truth.set_param(F=config[\"F_truth\"] + 2 * np.sin(2 * np.pi * i / mon_per_ann))\n",
    "        X_step, Y_step, t_step = M_truth.run(config[\"si\"], config[\"si\"] * mon_period)\n",
    "        X_truth = np.concatenate((X_truth, X_step[1:None, ...]))\n",
    "        Y_truth = np.concatenate((Y_truth, Y_step[1:None, ...]))\n",
    "        t_truth = np.concatenate((t_truth, t_truth[-1] + t_step[1:None]))\n",
    "else:\n",
    "    X_truth, Y_truth, t_truth = M_truth.run(config[\"si\"], config[\"si\"] * config[\"ns\"])\n",
    "\n",
    "# generate climatological background (temporal) covariance for 2-scale L96 model\n",
    "B_clim2 = np.cov(X_truth.T)\n",
    "# np.save(\"B_clim_L96.npy\", B_clim2)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(221)\n",
    "# Snapshot of X[k]\n",
    "plt.plot(M_truth.k, X_truth[-1, :], label=\"X\")\n",
    "plt.plot(M_truth.j / M_truth.J, Y_truth[-1, :], label=\"Y\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"k\")\n",
    "plt.title(\"$X,Y$ @ $t=N\\Delta t$\")\n",
    "plt.plot(M_truth.k, X_truth[0, :], \"k:\")\n",
    "plt.plot(M_truth.j / M_truth.J, Y_truth[0, :], \"k:\")\n",
    "plt.subplot(222)\n",
    "# Sample time-series X[0](t), Y[0](t)\n",
    "plt.plot(t_truth, X_truth[:, 0], label=\"X\")\n",
    "plt.plot(t_truth, Y_truth[:, 0], label=\"Y\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.title(\"$X[0,t]$, $Y[0,t]$\")\n",
    "plt.subplot(223)\n",
    "# Full model history of X\n",
    "plt.contourf(M_truth.k, t_truth, X_truth)\n",
    "plt.colorbar(orientation=\"horizontal\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.title(\"$X(k,t)$\")\n",
    "plt.subplot(224)\n",
    "# Full model history of Y\n",
    "plt.contourf(M_truth.j / M_truth.J, t_truth, Y_truth)\n",
    "plt.colorbar(orientation=\"horizontal\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"t\")\n",
    "plt.title(\"$Y(k,t)$\")\n",
    "\n",
    "# generate climatological background covariance for 1-scale L96 model\n",
    "M_1s = L96s(config[\"K\"], F=config[\"F_truth\"], dt=config[\"dt\"], method=RK4)\n",
    "M_1s.set_state(X_init)\n",
    "X1_truth, t1_truth = M_1s.run(config[\"si\"] * config[\"ns\"])\n",
    "B_clim1 = np.cov(X1_truth.T)\n",
    "# np.save('B_clim_1s.npy', B_clim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f14f72",
   "metadata": {},
   "source": [
    "# 3. Generate synthetic observations\n",
    "\n",
    "Here we generate a set of sparse observations by sampling from the X_truth run, and adding random Gaussian noise. \n",
    "\n",
    "The covariance matrix over the observations 'R' (used to express the uncertainty in the observations during DA) is given as a diagonal matrix with entries defined by config['obs_sigma']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc69e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the \"truth\" to generate observations at certain times (t_obs) and locations (l_obs)\n",
    "t_obs = np.tile(\n",
    "    np.arange(\n",
    "        config[\"obs_freq\"], config[\"ns_da\"] + config[\"obs_freq\"], config[\"obs_freq\"]\n",
    "    ),\n",
    "    [int(config[\"K\"] * config[\"obs_density\"]), 1],\n",
    ").T\n",
    "l_obs = np.zeros(t_obs.shape, dtype=\"int\")\n",
    "for i in range(l_obs.shape[0]):\n",
    "    l_obs[i, :] = rng.choice(\n",
    "        config[\"K\"], int(config[\"K\"] * config[\"obs_density\"]), replace=False\n",
    "    )\n",
    "X_obs = X_truth[t_obs, l_obs] + config[\"obs_sigma\"] * rng.standard_normal(l_obs.shape)\n",
    "\n",
    "# Calculated observation covariance matrix, assuming independent observations\n",
    "R = config[\"obs_sigma\"] ** 2 * np.eye(int(config[\"K\"] * config[\"obs_density\"]))\n",
    "\n",
    "plt.figure(figsize=[10, 6])\n",
    "plt.plot(range(1000, 1500), X_truth[1000:1500, 0], label=\"truth\")\n",
    "plt.scatter(\n",
    "    t_obs[100:150, 0],\n",
    "    find_obs(0, X_obs, t_obs, l_obs, [t_obs[100, 0], t_obs[150, 0]]),\n",
    "    color=\"k\",\n",
    "    label=\"obs\",\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72bc6d5",
   "metadata": {},
   "source": [
    "# 4. Apply localization to the background model covariance\n",
    "\n",
    "The covariance of the model climatology was computed a-priori form a long run. In this step we apply the localized weighting to the covariance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import DA_methods\n",
    "\n",
    "importlib.reload(DA_methods)\n",
    "\n",
    "# load pre-calculated climatological background covariance matrix from a long simulation\n",
    "# B_clim1 = np.load(\"B_clim_L96s.npy\")\n",
    "B_loc, W_clim = cov_loc(B_clim1, loc=config[\"B_loc\"])\n",
    "\n",
    "# B_clim2 = np.load(\"B_clim_L96.npy\")\n",
    "B_corr1 = np.zeros(B_clim1.shape)\n",
    "B_corr2 = np.zeros(B_clim2.shape)\n",
    "for i in range(B_clim1.shape[0]):\n",
    "    for j in range(B_clim1.shape[1]):\n",
    "        B_corr1[i, j] = B_clim1[i, j] / np.sqrt(B_clim1[i, i] * B_clim1[j, j])\n",
    "        B_corr2[i, j] = B_clim2[i, j] / np.sqrt(B_clim2[i, i] * B_clim2[j, j])\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(121)\n",
    "plt.contourf(B_corr1, cmap=\"bwr\", extend=\"both\", levels=np.linspace(-0.95, 0.95, 20))\n",
    "plt.colorbar()\n",
    "plt.title(\"Background correlation matrix 1-scale L96\")\n",
    "plt.subplot(122)\n",
    "plt.contourf(B_corr2, cmap=\"bwr\", extend=\"both\", levels=np.linspace(-0.95, 0.95, 20))\n",
    "plt.colorbar()\n",
    "plt.title(\"Background correlation matrix 2-scale L96\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a249a",
   "metadata": {},
   "source": [
    "# 4. Run Data Assimilation\n",
    "\n",
    "During each DA cycle produce an updated state estimate for all K grid points, and all n number of ensemble members\n",
    "\n",
    "Inputs: prior estimate (the forecast at time t for all K grid points, and all n number of ensemble members)\n",
    "        observations (N observations)\n",
    "        operator matrix (N x K)\n",
    "        covariance over obs (N x N)\n",
    "        covariance over model (K x K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# set up array to store DA increments\n",
    "X_inc = np.zeros(\n",
    "    (int(config[\"ns_da\"] / config[\"DA_freq\"]), config[\"K\"], config[\"nens\"])\n",
    ")\n",
    "if config[\"DA\"] == \"3DVar\":\n",
    "    X_inc = np.squeeze(X_inc)\n",
    "t_DA = np.zeros(int(config[\"ns_da\"] / config[\"DA_freq\"]))\n",
    "\n",
    "# initialize ensemble with perturbations\n",
    "i_t = 0\n",
    "ensX = X_init[None, :, None] + rng.standard_normal((1, config[\"K\"], config[\"nens\"]))\n",
    "X_post = ensX[0, ...]\n",
    "\n",
    "\n",
    "if config[\"param_DA\"]:\n",
    "    mean_param = np.zeros(\n",
    "        (int(config[\"ns_da\"] / config[\"DA_freq\"]), len(config[\"GCM_param\"]))\n",
    "    )\n",
    "    spread_param = np.zeros(\n",
    "        (int(config[\"ns_da\"] / config[\"DA_freq\"]), len(config[\"GCM_param\"]))\n",
    "    )\n",
    "    param_scale = config[\"param_sd\"]\n",
    "    W = np.ones(\n",
    "        (config[\"K\"] + len(config[\"GCM_param\"]), config[\"K\"] + len(config[\"GCM_param\"]))\n",
    "    )\n",
    "    W[0 : config[\"K\"], 0 : config[\"K\"]] = W_clim\n",
    "else:\n",
    "    W = W_clim\n",
    "    param_scale = np.zeros(config[\"GCM_param\"].shape)\n",
    "\n",
    "ens_param = np.zeros((len(config[\"GCM_param\"]), config[\"nens\"]))\n",
    "for i in range(len(config[\"GCM_param\"])):\n",
    "    ens_param[i, :] = config[\"GCM_param\"][i] + rng.normal(\n",
    "        scale=param_scale[i], size=config[\"nens\"]\n",
    "    )\n",
    "\n",
    "# DA cycles\n",
    "for cycle in np.arange(0, config[\"ns_da\"] / config[\"DA_freq\"], dtype=\"int\"):\n",
    "    # for cycle in np.arange(0,1,dtype='int'):\n",
    "\n",
    "    # set up array to store model forecast for each DA cycle\n",
    "    ensX_fcst = np.zeros((config[\"DA_freq\"] + 1, config[\"K\"], config[\"nens\"]))\n",
    "    # model forecast for next DA cycle\n",
    "    for n in range(config[\"nens\"]):\n",
    "        ensX_fcst[..., n] = GCM(\n",
    "            X_post[0 : config[\"K\"], n],\n",
    "            config[\"F_fcst\"],\n",
    "            config[\"dt\"],\n",
    "            config[\"DA_freq\"],\n",
    "            ens_param[:, n],\n",
    "        )[0]\n",
    "    i_t = i_t + config[\"DA_freq\"]\n",
    "\n",
    "    # get prior/background from the forecast\n",
    "    X_prior = ensX_fcst[-1, ...]  # get prior from model integration\n",
    "\n",
    "    # call DA\n",
    "    t_DA[cycle] = t_truth[i_t]\n",
    "    if config[\"DA\"] == \"EnKF\":\n",
    "        H = ObsOp(config[\"K\"], l_obs, t_obs, i_t)\n",
    "        # augment state vector with parameters when doing parameter estimation\n",
    "        if config[\"param_DA\"]:\n",
    "            H = np.concatenate(\n",
    "                (H, np.zeros((H.shape[0], len(config[\"GCM_param\"])))), axis=-1\n",
    "            )\n",
    "            X_prior = np.concatenate((X_prior, ens_param))\n",
    "        B_ens = np.cov(X_prior)\n",
    "        B_ens_loc = B_ens * W\n",
    "        X_post = DA_methods.EnKF(X_prior, X_obs[t_obs == i_t], H, R, B_ens_loc)\n",
    "\n",
    "        X_post[0 : config[\"K\"], :] = DA_methods.ens_inflate(\n",
    "            X_post[0 : config[\"K\"], :],\n",
    "            X_prior[0 : config[\"K\"], :],\n",
    "            config[\"inflate_opt\"],\n",
    "            config[\"inflate_factor\"],\n",
    "        )\n",
    "        if config[\"param_DA\"]:\n",
    "            X_post[-len(config[\"GCM_param\"]) : None, :] = DA_methods.ens_inflate(\n",
    "                X_post[-len(config[\"GCM_param\"]) : None, :],\n",
    "                X_prior[-len(config[\"GCM_param\"]) : None, :],\n",
    "                config[\"param_inflate\"],\n",
    "                config[\"param_inf_factor\"],\n",
    "            )\n",
    "            ens_param = X_post[-len(config[\"GCM_param\"]) : None, :]\n",
    "    elif config[\"DA\"] == \"HyEnKF\":\n",
    "        H = ObsOp(config[\"K\"], l_obs, t_obs, i_t)\n",
    "        B_ens = (\n",
    "            np.cov(X_prior) * (1 - config[\"hybrid_factor\"])\n",
    "            + B_clim1 * config[\"hybrid_factor\"]\n",
    "        )\n",
    "        B_ens_loc = B_ens * W\n",
    "        X_post = DA_methods.EnKF(X_prior, X_obs[t_obs == i_t], H, R, B_ens_loc)\n",
    "        X_post = DA_methods.ens_inflate(\n",
    "            X_post, X_prior, config[\"inflate_opt\"], config[\"inflate_factor\"]\n",
    "        )\n",
    "    elif config[\"DA\"] == \"3DVar\":\n",
    "        X_prior = np.squeeze(X_prior)\n",
    "        H = ObsOp(config[\"K\"], l_obs, t_obs, i_t)\n",
    "        X_post = DA_methods.Lin3dvar(X_prior, X_obs[t_obs == i_t], H, R, B_loc, 3)\n",
    "    elif config[\"DA\"] == \"Replace\":\n",
    "        X_post = X_prior\n",
    "        X_post[l_obs[t_obs == i_t]] = X_obs[t_obs == i_t]\n",
    "    elif config[\"DA\"] == \"None\":\n",
    "        X_post = X_prior\n",
    "\n",
    "    if not config[\"DA\"] == \"None\":\n",
    "        X_inc[cycle, ...] = (\n",
    "            np.squeeze(X_post[0 : config[\"K\"], ...]) - X_prior[0 : config[\"K\"], ...]\n",
    "        )  # get current increments\n",
    "        # get posterior info about the estimated parameters\n",
    "        if config[\"param_DA\"]:\n",
    "            mean_param[cycle, :] = ens_param.mean(axis=-1)\n",
    "            spread_param[cycle, :] = ens_param.std(axis=-1)\n",
    "\n",
    "    # reset initial conditions for next DA cycle\n",
    "    ensX_fcst[-1, :, :] = X_post[0 : config[\"K\"], :]\n",
    "    ensX = np.concatenate((ensX, ensX_fcst[1:None, ...]))\n",
    "\n",
    "print(\"time to complete DA: \", round(time.time() - t0, 2), \" (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d243a69",
   "metadata": {},
   "source": [
    "# 5. Post processing and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanX = np.mean(ensX, axis=-1)\n",
    "clim = np.max(np.abs(meanX - X_truth[0 : (config[\"ns_da\"] + 1), :]))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "ch = axes[0, 0].contourf(\n",
    "    M_truth.k,\n",
    "    t_truth[0 : (config[\"ns_da\"] + 1)],\n",
    "    meanX - X_truth[0 : (config[\"ns_da\"] + 1), :],\n",
    "    cmap=\"bwr\",\n",
    "    vmin=-clim,\n",
    "    vmax=clim,\n",
    "    extend=\"neither\",\n",
    ")\n",
    "plt.colorbar(ch, ax=axes[0, 0], orientation=\"horizontal\")\n",
    "axes[0, 0].set_xlabel(\"s\")\n",
    "axes[0, 0].set_ylabel(\"t\", rotation=0)\n",
    "axes[0, 0].set_title(\"X - X_truth\")\n",
    "axes[0, 1].plot(\n",
    "    t_truth[0 : (config[\"ns_da\"] + 1)],\n",
    "    np.sqrt(((meanX - X_truth[0 : (config[\"ns_da\"] + 1), :]) ** 2).mean(axis=-1)),\n",
    "    label=\"RMSE\",\n",
    ")\n",
    "axes[0, 1].plot(\n",
    "    t_truth[0 : (config[\"ns_da\"] + 1)],\n",
    "    np.mean(np.std(ensX, axis=-1), axis=-1),\n",
    "    label=\"Spread\",\n",
    ")\n",
    "axes[0, 1].plot(\n",
    "    t_truth[0 : (config[\"ns_da\"] + 1)],\n",
    "    config[\"obs_sigma\"] * np.ones((config[\"ns_da\"] + 1)),\n",
    "    label=\"Obs error\",\n",
    ")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xlabel(\"time\")\n",
    "axes[0, 1].set_title(\"RMSE (X - X_truth)\")\n",
    "axes[0, 1].grid(which=\"both\", linestyle=\"--\")\n",
    "\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k,\n",
    "    np.sqrt(((meanX - X_truth[0 : (config[\"ns_da\"] + 1), :]) ** 2).mean(axis=0)),\n",
    "    label=\"RMSE\",\n",
    ")\n",
    "X_inc_ave = X_inc / config[\"DA_freq\"] / config[\"si\"]\n",
    "axes[0, 2].plot(M_truth.k, X_inc_ave.mean(axis=(0, -1)), label=\"Inc\")\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k, running_ave(X_inc_ave.mean(axis=(0, -1)), 7), label=\"Inc Ave\"\n",
    ")\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k,\n",
    "    np.ones(M_truth.k.shape) * (config[\"F_fcst\"] - config[\"F_truth\"]),\n",
    "    label=\"F_bias\",\n",
    ")\n",
    "axes[0, 2].plot(\n",
    "    M_truth.k,\n",
    "    np.ones(M_truth.k.shape) * (X_inc / config[\"DA_freq\"] / config[\"si\"]).mean(),\n",
    "    \"k:\",\n",
    "    label=\"Ave Inc\",\n",
    ")\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].set_xlabel(\"s\")\n",
    "axes[0, 2].set_title(\"Increments\")\n",
    "axes[0, 2].grid(which=\"both\", linestyle=\"--\")\n",
    "\n",
    "# X_inc_ave=(X_inc/config['DA_freq']/config['si']).mean(axis=(1,2)).\\\n",
    "#         reshape(int(config['ns_da']/ann_period),int(ann_period/config['DA_freq'])).mean(axis=0)\n",
    "# axes[0,2].plot(np.arange(ann_period/config['DA_freq']),X_inc_ave,label='Inc')\n",
    "# axes[0,2].plot(np.arange(ann_period/config['DA_freq']),running_ave(X_inc_ave,10),label='Inc Ave');\n",
    "# axes[0,2].plot(np.arange(0,ann_period/config['DA_freq'],mon_period/config['DA_freq']),\n",
    "#                -2*np.sin(2*np.pi*np.arange(mon_per_ann)/mon_per_ann),label='F_bias')\n",
    "# axes[0,2].legend()\n",
    "# axes[0,2].set_xlabel('\"annual cycle\"'); axes[0,2].set_title('Increments');\n",
    "# axes[0,2].grid(which='both',linestyle='--')\n",
    "\n",
    "plot_start, plot_end = 1000, 1500\n",
    "plot_start_DA, plot_end_DA = int(plot_start / config[\"DA_freq\"]), int(\n",
    "    plot_end / config[\"DA_freq\"]\n",
    ")\n",
    "plot_x = 0\n",
    "axes[1, 0].plot(\n",
    "    t_truth[plot_start:plot_end], X_truth[plot_start:plot_end, plot_x], label=\"truth\"\n",
    ")\n",
    "axes[1, 0].plot(\n",
    "    t_truth[plot_start:plot_end], meanX[plot_start:plot_end, plot_x], label=\"forecast\"\n",
    ")\n",
    "axes[1, 0].scatter(\n",
    "    t_DA[plot_start_DA - 1 : plot_end_DA - 1],\n",
    "    find_obs(plot_x, X_obs, t_obs, l_obs, [plot_start, plot_end]),\n",
    "    label=\"obs\",\n",
    ")\n",
    "axes[1, 0].grid(which=\"both\", linestyle=\"--\")\n",
    "axes[1, 0].set_xlabel(\"time\")\n",
    "axes[1, 0].set_title(\"k=\" + str(plot_x + 1) + \" truth and forecast\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "if config[\"param_DA\"]:\n",
    "    for i, c in zip(np.arange(len(config[\"GCM_param\"]), 0, -1), [\"r\", \"b\", \"g\", \"k\"]):\n",
    "        axes[1, 1].plot(\n",
    "            t_DA,\n",
    "            running_ave(mean_param[:, i - 1], 100),\n",
    "            c + \"-\",\n",
    "            label=\"C{} {:3f}\".format(\n",
    "                i - 1, mean_param[int(len(t_DA) / 2) : None, i - 1].mean()\n",
    "            ),\n",
    "        )\n",
    "        axes[1, 1].plot(\n",
    "            t_DA,\n",
    "            running_ave(mean_param[:, i - 1] + spread_param[:, i - 1], 100),\n",
    "            c + \":\",\n",
    "            label=\"SD {:3f}\".format(\n",
    "                spread_param[int(len(t_DA) / 2) : None, i - 1].mean()\n",
    "            ),\n",
    "        )\n",
    "        axes[1, 1].plot(\n",
    "            t_DA,\n",
    "            running_ave(mean_param[:, i - 1] - spread_param[:, i - 1], 100),\n",
    "            c + \":\",\n",
    "        )\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(which=\"both\", linestyle=\"--\")\n",
    "\n",
    "else:\n",
    "    axes[1, 1].axis(\"off\")\n",
    "\n",
    "axes[1, 2].text(\n",
    "    0.1,\n",
    "    0.2,\n",
    "    \"RMSE={:3f}\\nSpread={:3f}\\nGCM param={}\\nDA={},{}\\nDA_freq={}\\nB_loc={}\\ninflation={},{}\\nobs_density={}\\nobs_sigma={}\\nobs_freq={}\".format(\n",
    "        np.sqrt(((meanX - X_truth[0 : (config[\"ns_da\"] + 1), :]) ** 2).mean()),\n",
    "        np.mean(np.std(ensX, axis=-1)),\n",
    "        config[\"DA\"],\n",
    "        config[\"GCM_param\"],\n",
    "        config[\"nens\"],\n",
    "        config[\"DA_freq\"],\n",
    "        config[\"B_loc\"],\n",
    "        config[\"inflate_opt\"],\n",
    "        config[\"inflate_factor\"],\n",
    "        config[\"obs_density\"],\n",
    "        config[\"obs_sigma\"],\n",
    "        config[\"obs_freq\"],\n",
    "    ),\n",
    "    fontsize=15,\n",
    ")\n",
    "axes[1, 2].axis(\"off\")\n",
    "\n",
    "if config[\"save_fig\"]:\n",
    "    data_path = \"./DA_data/\"\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    exp_number = np.random.randint(1, 10000)\n",
    "    f = open(data_path + \"config_{0}.txt\".format(exp_number), \"w\")\n",
    "    f.write(data_path + str(config))\n",
    "    f.close()\n",
    "    plt.savefig(data_path + \"fig_{0}.png\".format(exp_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65993585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save DA output for further analysis\n",
    "if config[\"save_data\"]:\n",
    "    config_str = (\n",
    "        \"K_\"\n",
    "        + str(config[\"K\"])\n",
    "        + \"_J_\"\n",
    "        + str(config[\"J\"])\n",
    "        + \"_obs_freq_\"\n",
    "        + str(config[\"obs_freq\"])\n",
    "        + \"_F_truth_\"\n",
    "        + str(config[\"F_truth\"])\n",
    "        + \"_F_fcst_\"\n",
    "        + str(config[\"F_fcst\"])\n",
    "        + \"_ns_da_\"\n",
    "        + str(config[\"ns_da\"])\n",
    "        + \"_ns_\"\n",
    "        + str(config[\"ns\"])\n",
    "        + \"_ns_spinup_\"\n",
    "        + str(config[\"ns_spinup\"])\n",
    "        + \"_dt_\"\n",
    "        + str(config[\"dt\"])\n",
    "        + \"_si_\"\n",
    "        + str(config[\"si\"])\n",
    "        + \"_B_loc_\"\n",
    "        + str(config[\"B_loc\"])\n",
    "        + \"_DA_\"\n",
    "        + str(config[\"DA\"])\n",
    "        + \"_nens_\"\n",
    "        + str(config[\"nens\"])\n",
    "        + \"_inflate_opt_\"\n",
    "        + str(config[\"inflate_opt\"])\n",
    "        + \"_inflate_factor_\"\n",
    "        + str(config[\"inflate_factor\"])\n",
    "        + \"_hybrid_factor_\"\n",
    "        + str(config[\"hybrid_factor\"])\n",
    "        + \"_obs_density_\"\n",
    "        + str(config[\"obs_density\"])\n",
    "        + \"_DA_freq_\"\n",
    "        + str(config[\"DA_freq\"])\n",
    "        + \"_obs_sigma_\"\n",
    "        + str(config[\"obs_sigma\"])\n",
    "        + \".npz\"\n",
    "    )\n",
    "\n",
    "    data_path = \"./DA_data/\"\n",
    "    if not os.path.isdir(data_path):\n",
    "        os.mkdir(data_path)\n",
    "\n",
    "    # np.savez(data_path+config_str,meanX=meanX,ensX=ensX,X_truth=X_truth,X_inc=X_inc,X_inc_ave=X_inc_ave)\n",
    "    np.savez(data_path + config_str, meanX=meanX, X_truth=X_truth, X_inc_ave=X_inc_ave)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
